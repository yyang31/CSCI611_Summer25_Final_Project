{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b632b1b9",
   "metadata": {},
   "source": [
    "# Safety First - Hard Hat Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d6c0c4f",
   "metadata": {},
   "source": [
    "---\n",
    "## Project Proposal\n",
    "\n",
    "### Team Member\n",
    "\n",
    "- Stanley Yang\n",
    "- Lennard Vanderspek \n",
    " \n",
    "### Description\n",
    "\n",
    "Workplace safety is very important but often overlooked. Most important way to keep workers \n",
    "safe is by wearing a hard hat. This project will utilize Convolution Neural Net and YOLO to help \n",
    "detect whether workers are wearing a hard hat or not.\n",
    "\n",
    "#### Machine Learning Topics Used:\n",
    "\n",
    "- Convolution Neural Net \n",
    "- YOLO \n",
    " \n",
    "### Expected Outcome\n",
    "\n",
    "A trained model that is able to detect whether workers are wearing a hard hat or not by \n",
    "implementing YOLO using PyTorch. We are hoping to utilize the model to bring more awareness \n",
    "to workplace safety."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c20f2620",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Use CUDA if Available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "922a8ad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# check if CUDA is available\n",
    "train_on_gpu = torch.cuda.is_available()\n",
    "\n",
    "if not train_on_gpu:\n",
    "    print(\"CUDA is not available.  Training on CPU ...\")\n",
    "else:\n",
    "    print(\"CUDA is available!  Training on GPU ...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01756944",
   "metadata": {},
   "source": [
    "---\n",
    "## Load the Data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "926ee1bd",
   "metadata": {},
   "source": [
    "### Create Custom Dataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0690d23d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "import cv2\n",
    "import torchvision.transforms as transforms\n",
    "import os\n",
    "\n",
    "# specify the image classes\n",
    "classes = ['head', 'helmet', 'person']\n",
    "num_classes = len(classes)\n",
    "\n",
    "def yolo_collate_fn(batch):\n",
    "    images, labels = zip(*batch)\n",
    "    images = torch.stack(images, dim=0)\n",
    "    # Pad labels to the max number of objects in the batch\n",
    "    max_objs = max(label.shape[0] for label in labels)\n",
    "    padded_labels = []\n",
    "    for label in labels:\n",
    "        pad_size = max_objs - label.shape[0]\n",
    "        if pad_size > 0:\n",
    "            # Pad with -1\n",
    "            padded = torch.nn.functional.pad(label, (0, 0, 0, pad_size), value=-1)\n",
    "        else:\n",
    "            padded = label\n",
    "        padded_labels.append(padded)\n",
    "    labels = torch.stack(padded_labels, dim=0)\n",
    "    return images, labels\n",
    "\n",
    "class custom_dataset(Dataset):\n",
    "    def __init__(self, image_path, label_path, transform:transforms = None, number_image_to_load=None):\n",
    "        self.image_path = image_path\n",
    "        self.label_path = label_path\n",
    "        self.transform = transform\n",
    "\n",
    "        if number_image_to_load is None:\n",
    "            # Load images and labels\n",
    "            self.images = sorted(os.listdir(image_path))\n",
    "            self.labels = sorted(os.listdir(label_path))\n",
    "        else:\n",
    "            # Load a limited number of images and labels\n",
    "            self.images = sorted(os.listdir(image_path))[:number_image_to_load]\n",
    "            self.labels = sorted(os.listdir(label_path))[:number_image_to_load]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image_name = self.images[index]\n",
    "        label_name = self.labels[index]\n",
    "\n",
    "        # Ensure the label file corresponds to the image file\n",
    "        if not label_name.startswith(image_name.split('.')[0]):\n",
    "            raise ValueError(f\"Label file {label_name} does not match image file {image_name}\")\n",
    "\n",
    "        # Load the image\n",
    "        image = cv2.imread(os.path.join(self.image_path, image_name))\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        # Load the label\n",
    "        # Example INPUT with format [class_id, x_center, y_center, width, height]\n",
    "        # 1 0.502 0.6506024096385542 0.032 0.060240963855421686\n",
    "        #\n",
    "        # Example converted OUTPUT with format [x_center, y_center, width, height, confidence, class_0, class_1, class_2]\n",
    "        # 0.502 0.6506024096385542 0.032 0.060240963855421686 1 0.0 1.0 0.0\n",
    "        labels = []\n",
    "        with open(os.path.join(self.label_path, label_name), 'r') as f:\n",
    "            for line in f.readlines():\n",
    "                parts = line.strip().split()\n",
    "                if len(parts) != 5:\n",
    "                    print(f\"Skipping invalid label line: {line.strip()}\")\n",
    "                    continue\n",
    "                class_id = int(parts[0])\n",
    "                x_center = float(parts[1])\n",
    "                y_center = float(parts[2])\n",
    "                width = float(parts[3])\n",
    "                height = float(parts[4])\n",
    "\n",
    "                # Confidence = 1.0 for ground truth\n",
    "                conf = 1.0\n",
    "\n",
    "                # One-hot encoding for class\n",
    "                one_hot_class = [0.0] * num_classes\n",
    "                one_hot_class[class_id] = 1.0\n",
    "\n",
    "                # Final label vector\n",
    "                labels.append([x_center, y_center, width, height, conf] + one_hot_class)\n",
    "\n",
    "        return image, torch.tensor(labels, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52d11681",
   "metadata": {},
   "source": [
    "### Load the Data using Custom Dataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "410e0444",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "\n",
    "# TODO: Need to adjust the default values\n",
    "\n",
    "# number of subprocesses to use for data loading\n",
    "num_workers = 0\n",
    "\n",
    "# how many samples per batch to load\n",
    "batch_size = 20\n",
    "\n",
    "# percentage of training set to use as validation\n",
    "valid_size = 0.2\n",
    "\n",
    "# number of data to load (None for all)\n",
    "train_data_to_load = None\n",
    "test_data_to_load = None\n",
    "\n",
    "# resize the images to this size\n",
    "# smaller size means faster training\n",
    "# larger size means better detection on small objects\n",
    "#\n",
    "# if changed, need to update linear layer input size in the model\n",
    "image_size = (200, 200)\n",
    "\n",
    "# convert data to a normalized torch.FloatTensor\n",
    "transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "        transforms.Resize(image_size),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# load the data set\n",
    "train_data = custom_dataset(\n",
    "    image_path=\"data-yolo-v7/train/images\",\n",
    "    label_path=\"data-yolo-v7/train/labels\",\n",
    "    transform=transform,\n",
    "    number_image_to_load=train_data_to_load,\n",
    ")\n",
    "test_data = custom_dataset(\n",
    "    image_path=\"data-yolo-v7/test/images\",\n",
    "    label_path=\"data-yolo-v7/test/labels\",\n",
    "    transform=transform,\n",
    "    number_image_to_load=test_data_to_load,\n",
    ")\n",
    "num_train = len(train_data)\n",
    "indices = list(range(num_train))\n",
    "np.random.shuffle(indices)\n",
    "split = int(np.floor(valid_size * num_train))\n",
    "train_idx, valid_idx = indices[split:], indices[:split]\n",
    "\n",
    "# define samplers for obtaining training and validation batches\n",
    "train_sampler = SubsetRandomSampler(train_idx)\n",
    "valid_sampler = SubsetRandomSampler(valid_idx)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_data,\n",
    "    batch_size=batch_size,\n",
    "    sampler=train_sampler,\n",
    "    num_workers=num_workers,\n",
    "    collate_fn=yolo_collate_fn,\n",
    ")\n",
    "valid_loader = torch.utils.data.DataLoader(\n",
    "    train_data,\n",
    "    batch_size=batch_size,\n",
    "    sampler=valid_sampler,\n",
    "    num_workers=num_workers,\n",
    "    collate_fn=yolo_collate_fn,\n",
    ")\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    test_data,\n",
    "    batch_size=batch_size,\n",
    "    num_workers=num_workers,\n",
    "    collate_fn=yolo_collate_fn,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a467dbc0",
   "metadata": {},
   "source": [
    "### Custom Bounding Box Drawing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0d51626",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.patches as patches\n",
    "\n",
    "\n",
    "def draw_bounding_boxes(ax, labels, img_size, class_names, color=\"lime\"):\n",
    "    h, w = img_size\n",
    "\n",
    "    for box in labels:\n",
    "        if box[0] == -1:\n",
    "            continue  # skip padding\n",
    "\n",
    "        x_center, y_center, box_w, box_h = box[:4]\n",
    "        class_id = torch.argmax(box[5:]).item()\n",
    "\n",
    "        # Convert from normalized center coords to top-left corner\n",
    "        x = (x_center - box_w / 2) * w\n",
    "        y = (y_center - box_h / 2) * h\n",
    "        width = box_w * w\n",
    "        height = box_h * h\n",
    "\n",
    "        # Draw rectangle\n",
    "        rect = patches.Rectangle(\n",
    "            (x, y), width, height, linewidth=2, edgecolor=color, facecolor=\"none\"\n",
    "        )\n",
    "        ax.add_patch(rect)\n",
    "\n",
    "        # Add label text\n",
    "        label_text = f\"{class_names[class_id]} {box[4].item():.2f}\"\n",
    "        ax.text(\n",
    "            x,\n",
    "            y - 5,\n",
    "            label_text,\n",
    "            color=color,\n",
    "            backgroundcolor=\"black\",\n",
    "            fontsize=10,\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7c16ab5",
   "metadata": {},
   "source": [
    "### Visualize a Batch of Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd3146d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# helper function to un-normalize and display an image\n",
    "def imshow(img):\n",
    "    img = img / 2 + 0.5  # unnormalize\n",
    "    plt.imshow(np.transpose(img, (1, 2, 0)))  # convert from Tensor image\n",
    "\n",
    "# obtain one batch of training images\n",
    "dataiter = iter(train_loader)\n",
    "\n",
    "#images, labels = dataiter.next() #python, torchvision version match issue\n",
    "images, labels = next(dataiter)\n",
    "images = images.numpy() # convert images to numpy for display\n",
    "\n",
    "# plot the images in the batch, along with the corresponding labels\n",
    "fig = plt.figure(figsize=(25, 25))\n",
    "\n",
    "# display 2 images\n",
    "for idx in range(2):\n",
    "    ax = fig.add_subplot(1, 2, idx+1, xticks=[], yticks=[])\n",
    "    image_np = np.transpose(images[idx], (1, 2, 0))  # shape: (H, W, C)\n",
    "    image_np = image_np * 0.5 + 0.5  # unnormalize\n",
    "    ax.imshow(image_np)\n",
    "\n",
    "    label = labels[idx]\n",
    "    valid_labels = label[~(label == -1).all(dim=1)]\n",
    "    draw_bounding_boxes(ax, valid_labels, img_size=(image_np.shape[0], image_np.shape[1]), class_names=classes)\n",
    "\n",
    "    # filter out padding (rows where all values are -1)\n",
    "    valid_labels = label[~(label == -1).all(axis=1)]\n",
    "    ax.set_title(f'Labels:\\n{valid_labels}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8a7eed3",
   "metadata": {},
   "source": [
    "### View an Image in More Detail\n",
    "\n",
    "Showing the normalized red, green, and blue (RGB) color channels as three separate, gray scale intensity images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acb65f3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "rgb_img = np.squeeze(images[0])\n",
    "channels = [\"red channel\", \"green channel\", \"blue channel\"]\n",
    "step = 10  # visualizing in steps of 10 pixels due to large image size\n",
    "\n",
    "fig = plt.figure(figsize=(36, 36))\n",
    "for idx in np.arange(rgb_img.shape[0]):\n",
    "    ax = fig.add_subplot(1, 3, idx + 1)\n",
    "    img = rgb_img[idx]\n",
    "    ax.imshow(img, cmap=\"gray\")\n",
    "    ax.set_title(channels[idx])\n",
    "    width, height = img.shape\n",
    "    thresh = img.max() / 2.5\n",
    "    for x in range(0, width, step):\n",
    "        for y in range(0, height, step):\n",
    "            val = round(img[x][y], 2) if img[x][y] != 0 else 0\n",
    "            ax.annotate(\n",
    "                str(val),\n",
    "                xy=(y, x),\n",
    "                horizontalalignment=\"center\",\n",
    "                verticalalignment=\"center\",\n",
    "                size=8,\n",
    "                color=\"white\" if img[x][y] < thresh else \"black\",\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8b24e60",
   "metadata": {},
   "source": [
    "---\n",
    "## Define the Network Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42cc2079",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class yolo(nn.Module):\n",
    "    # default to predict 3 bounding boxes per image and 3 classes\n",
    "    def __init__(self, num_boxes=3, num_classes=3):\n",
    "        super(yolo, self).__init__()\n",
    "\n",
    "        self.num_boxes = num_boxes\n",
    "        self.num_classes = num_classes\n",
    "        self.output_dim = self.num_boxes * (\n",
    "            5 + self.num_classes\n",
    "        )  # 5 = [x, y, w, h, conf]\n",
    "\n",
    "        self.convolution_layers = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "\n",
    "            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "\n",
    "            nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "        )\n",
    "\n",
    "        self.detection_layers = nn.Sequential(\n",
    "            nn.Conv2d(256, 512, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(),\n",
    "            \n",
    "            nn.Linear(\n",
    "                512 * 25 * 25, self.num_boxes * (5 + self.num_classes)\n",
    "            ),  # 25 is because we have 3 max pooling layers reducing the size from 400x400 to x25\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = self.convolution_layers(x)\n",
    "        predictions = self.detection_layers(features)\n",
    "        predictions = predictions.view(-1, self.num_boxes, 5 + self.num_classes)\n",
    "        predictions[..., 4] = torch.sigmoid(predictions[..., 4])\n",
    "        return predictions\n",
    "\n",
    "\n",
    "# create a complete CNN\n",
    "model = yolo()\n",
    "print(model)\n",
    "\n",
    "# output total number of parameters\n",
    "print(\n",
    "    f\"Total parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad)}\"\n",
    ")\n",
    "\n",
    "# move tensors to GPU if CUDA is available\n",
    "if train_on_gpu:\n",
    "    model.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f2b414f",
   "metadata": {},
   "source": [
    "---\n",
    "## Specify Loss Function and Optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96b6f11c",
   "metadata": {},
   "source": [
    "### Create Custom Loss Function for YOLO\n",
    "\n",
    "YOLO loss function is defined as following ([reference](https://www.geeksforgeeks.org/computer-vision/yolov3-from-scratch-using-pytorch/)):\n",
    "\n",
    "![loss function](./images/yolo_loss_function.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76ae44ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Need to adjust the default values\n",
    "LAMBDA_COORD = 1.0\n",
    "LAMBDA_OBJ = 1.0\n",
    "LAMBDA_NOOBJ = 1.0\n",
    "LAMBDA_CLASS = 1.0\n",
    "CONFIDENCE_THRESHOLD = 0.3\n",
    "IOU_THRESHOLD = 0.3\n",
    "\n",
    "def calculate_iou(pred, target):\n",
    "    # pred and target are in [x_center, y_center, w, h]\n",
    "    pred_x1 = pred[0] - pred[2] / 2\n",
    "    pred_y1 = pred[1] - pred[3] / 2\n",
    "    pred_x2 = pred[0] + pred[2] / 2\n",
    "    pred_y2 = pred[1] + pred[3] / 2\n",
    "\n",
    "    target_x1 = target[0] - target[2] / 2\n",
    "    target_y1 = target[1] - target[3] / 2\n",
    "    target_x2 = target[0] + target[2] / 2\n",
    "    target_y2 = target[1] + target[3] / 2\n",
    "\n",
    "    inter_x1 = torch.max(pred_x1, target_x1)\n",
    "    inter_y1 = torch.max(pred_y1, target_y1)\n",
    "    inter_x2 = torch.min(pred_x2, target_x2)\n",
    "    inter_y2 = torch.min(pred_y2, target_y2)\n",
    "\n",
    "    inter_area = (inter_x2 - inter_x1).clamp(0) * (inter_y2 - inter_y1).clamp(0)\n",
    "    pred_area = (pred_x2 - pred_x1) * (pred_y2 - pred_y1)\n",
    "    target_area = (target_x2 - target_x1) * (target_y2 - target_y1)\n",
    "    union = pred_area + target_area - inter_area\n",
    "\n",
    "    return inter_area / union.clamp(min=1e-6)\n",
    "\n",
    "\n",
    "# TODO: Need to adjust the default values\n",
    "def custom_loss(predictions, targets, lambda_coord=LAMBDA_COORD, lambda_obj=LAMBDA_OBJ, lambda_noobj=LAMBDA_NOOBJ, lambda_class=LAMBDA_CLASS, confidence_threshold = CONFIDENCE_THRESHOLD, iou_threshold=IOU_THRESHOLD):\n",
    "    batch_size, _, _ = predictions.shape\n",
    "    total_loss = 0.0\n",
    "\n",
    "    for i in range(batch_size):\n",
    "        pred = predictions[i]         # [N, 5+C]\n",
    "        target = targets[i]           # [T, 5+C]\n",
    "        T = target.shape[0]\n",
    "\n",
    "        matched = []\n",
    "        used_targets = set()\n",
    "\n",
    "        for pred_i, pred_box in enumerate(pred):\n",
    "            # Skip predictions with low confidence\n",
    "            if pred_box[4] < confidence_threshold:\n",
    "                continue\n",
    "\n",
    "            top_iou = -1\n",
    "            top_index = -1\n",
    "            for target_i, target_box in enumerate(target):\n",
    "                if target_box[0] == -1 or target_i in used_targets:\n",
    "                    continue\n",
    "\n",
    "                iou = calculate_iou(pred_box[:4], target_box[:4])\n",
    "                if iou > top_iou:\n",
    "                    top_iou = iou\n",
    "                    top_index = target_i\n",
    "\n",
    "            if top_iou > iou_threshold:\n",
    "                matched.append((pred_i, top_index))\n",
    "                used_targets.add(top_index)\n",
    "            else:\n",
    "                # no-object confidence loss\n",
    "                noobj_conf_loss = torch.nn.functional.mse_loss(pred_box[4], torch.tensor(0.0, device=pred.device))\n",
    "                total_loss += lambda_noobj * noobj_conf_loss\n",
    "\n",
    "        for pred_i, target_i in matched:\n",
    "            pred_box = pred[pred_i]\n",
    "            target_box = target[target_i]\n",
    "\n",
    "            # box loss\n",
    "            box_loss = torch.nn.functional.mse_loss(pred_box[:4], target_box[:4])\n",
    "\n",
    "            # object confidence loss\n",
    "            conf_loss = torch.nn.functional.mse_loss(pred_box[4], target_box[4])\n",
    "\n",
    "            # classification loss\n",
    "            class_loss = torch.nn.functional.mse_loss(pred_box[5:], target_box[5:])\n",
    "\n",
    "            total_loss += lambda_coord * box_loss + lambda_obj * conf_loss + lambda_class * class_loss\n",
    "    \n",
    "    # Average the total loss over the batch\n",
    "    total_loss /= batch_size\n",
    "    return total_loss.clone().detach().requires_grad_(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5672cefd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# specify loss function (categorical cross-entropy)\n",
    "criterion = custom_loss\n",
    "\n",
    "# specify optimizer\n",
    "# TODO: adjust the learning rate\n",
    "# smaller lr means it requires more epochs to improve\n",
    "# larger lr means it may overshoot and become unstable\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e444a4ec",
   "metadata": {},
   "source": [
    "---\n",
    "## Train the Network\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ad01e97",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_loss_min = np.inf  # track change in validation loss\n",
    "total_epochs_run = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9752d8ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from datetime import timedelta\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# TODO: update the number of epochs to train the model\n",
    "# number of epochs to run for this iteration\n",
    "n_epochs = 5\n",
    "\n",
    "# load the model if this is a continuation of training\n",
    "if total_epochs_run > 0:\n",
    "    print(f\"Continuing training from epoch {total_epochs_run + 1}\")\n",
    "    model.load_state_dict(torch.load(\"model_trained.pt\", weights_only=True))\n",
    "\n",
    "for epoch in range(1, n_epochs + 1):\n",
    "    print(f\"Epoch {total_epochs_run + epoch}/{total_epochs_run + n_epochs}\")\n",
    "\n",
    "    # keep track of training and validation loss\n",
    "    train_loss = 0.0\n",
    "    valid_loss = 0.0\n",
    "\n",
    "    ###################\n",
    "    # train the model #\n",
    "    ###################\n",
    "    print(\"Training...\")\n",
    "    model.train()\n",
    "    for batch_idx, (images, targets) in enumerate(train_loader):\n",
    "        # move tensors to GPU if CUDA is available\n",
    "        if train_on_gpu:\n",
    "            images, targets = images.cuda(), targets.cuda()\n",
    "\n",
    "        # clear the gradients of all optimized variables\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward pass: compute predicted outputs by passing inputs to the model\n",
    "        output = model(images)\n",
    "\n",
    "        # calculate the batch loss\n",
    "        loss = criterion(output, targets)\n",
    "\n",
    "        # backward pass: compute gradient of the loss with respect to model parameters\n",
    "        loss.backward()\n",
    "\n",
    "        # perform a single optimization step (parameter update)\n",
    "        optimizer.step()\n",
    "\n",
    "        # update training loss\n",
    "        train_loss += loss.item() * images.size(0)\n",
    "\n",
    "    ######################\n",
    "    # validate the model #\n",
    "    ######################\n",
    "    print(\"Validating...\")\n",
    "    model.eval()\n",
    "    for batch_idx, (images, targets) in enumerate(valid_loader):\n",
    "        # move tensors to GPU if CUDA is available\n",
    "        if train_on_gpu:\n",
    "            images, targets = images.cuda(), targets.cuda()\n",
    "\n",
    "        # forward pass: compute predicted outputs by passing inputs to the model\n",
    "        output = model(images)\n",
    "\n",
    "        # calculate the batch loss\n",
    "        loss = criterion(output, targets)\n",
    "\n",
    "        # update average validation loss\n",
    "        valid_loss += loss.item() * images.size(0)\n",
    "\n",
    "    # calculate average losses\n",
    "    train_loss = train_loss / len(train_loader.sampler)\n",
    "    valid_loss = valid_loss / len(valid_loader.sampler)\n",
    "\n",
    "    # print training/validation statistics\n",
    "    print(f\"Training Loss: {train_loss}\")\n",
    "    print(f\"Validation Loss: {valid_loss}\")\n",
    "\n",
    "    # save model if validation loss has decreased\n",
    "    if valid_loss <= valid_loss_min:\n",
    "        print(f\"Validation loss has decreased from {valid_loss_min} to {valid_loss}\")\n",
    "        print(\"Saving model ...\")\n",
    "        torch.save(model.state_dict(), \"model_trained.pt\")\n",
    "        valid_loss_min = valid_loss\n",
    "\n",
    "    print(\"\\n\")\n",
    "\n",
    "total_epochs_run += n_epochs\n",
    "\n",
    "end_time = time.time()\n",
    "total_duration = end_time - start_time\n",
    "print(f\"Total Time for Training: {str(timedelta(seconds=int(total_duration)))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db9cca04",
   "metadata": {},
   "source": [
    "---\n",
    "## Load the Model with the Lowest Validation Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad5b40eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load('model_trained.pt', weights_only=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a51e78dd",
   "metadata": {},
   "source": [
    "---\n",
    "## Test the Trained Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fba5d97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# track test loss\n",
    "test_loss = 0.0\n",
    "class_correct = [0. for _ in range(num_classes)]\n",
    "class_total = [0. for _ in range(num_classes)]\n",
    "\n",
    "model.eval()\n",
    "for batch_idx, (data, target) in enumerate(test_loader):\n",
    "    if train_on_gpu:\n",
    "        data, target = data.cuda(), target.cuda()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = model(data)\n",
    "\n",
    "    # calculate the batch loss\n",
    "    loss = criterion(output, target)\n",
    "    test_loss += loss.item() * data.size(0)\n",
    "\n",
    "    batch_size = data.size(0)\n",
    "\n",
    "    for b in range(batch_size):\n",
    "        preds = output[b]       # shape: [N, 5 + num_classes]\n",
    "        labels = target[b]      # shape: [M, 5 + num_classes]\n",
    "\n",
    "        labels = labels[labels[:, 4] != -1]  # remove padding\n",
    "\n",
    "        for gt in labels:\n",
    "            gt_class = torch.argmax(gt[5:]).item()\n",
    "            best_iou = 0\n",
    "            best_pred_class = None\n",
    "\n",
    "            filtered_preds = preds[preds[:, 4] > CONFIDENCE_THRESHOLD]\n",
    "\n",
    "            for pred in filtered_preds:\n",
    "                iou = calculate_iou(pred[:4], gt[:4])\n",
    "                if iou > best_iou:\n",
    "                    best_iou = iou\n",
    "                    best_pred_class = torch.argmax(pred[5:]).item()\n",
    "\n",
    "            if best_iou > IOU_THRESHOLD:\n",
    "                class_total[gt_class] += 1\n",
    "                if best_pred_class == gt_class:\n",
    "                    class_correct[gt_class] += 1\n",
    "\n",
    "# average test loss\n",
    "test_loss = test_loss / len(test_loader.dataset)\n",
    "print('Test Loss: {:.6f}\\n'.format(test_loss))\n",
    "\n",
    "for i in range(num_classes):\n",
    "    if class_total[i] > 0:\n",
    "        print('Test Accuracy of %5s: %2d%% (%2d/%2d)' % (\n",
    "            classes[i], 100 * class_correct[i] / class_total[i],\n",
    "            class_correct[i], class_total[i]))\n",
    "    else:\n",
    "        print('Test Accuracy of %5s: N/A (no ground truth samples)' % (classes[i]))\n",
    "\n",
    "if sum(class_total) > 0:\n",
    "    print('\\nTest Accuracy (Overall): %2d%% (%2d/%2d)' % (\n",
    "        100. * sum(class_correct) / sum(class_total),\n",
    "        sum(class_correct), sum(class_total)))\n",
    "else:\n",
    "    print('\\nTest Accuracy (Overall): N/A (no valid ground truth boxes in test set)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "355f924e",
   "metadata": {},
   "source": [
    "## Visualize Sample Test Results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "368742d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "# Collect all test samples into memory for random access\n",
    "all_images = []\n",
    "all_targets = []\n",
    "\n",
    "for images, targets in test_loader:\n",
    "    all_images.extend(images)\n",
    "    all_targets.extend(targets)\n",
    "\n",
    "# Choose N random indices\n",
    "N = 4\n",
    "indices = random.sample(range(len(all_images)), k=N)\n",
    "\n",
    "fig, axs = plt.subplots(1, N, figsize=(5 * N, 5))\n",
    "if N == 1:\n",
    "    axs = [axs]\n",
    "\n",
    "model.eval()\n",
    "for i, idx in enumerate(indices):\n",
    "    image = all_images[idx].unsqueeze(0).to(next(model.parameters()).device)\n",
    "    target = all_targets[idx].to(image.device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = model(image)[0].cpu()\n",
    "\n",
    "    img = image[0].cpu().permute(1, 2, 0).numpy()\n",
    "    img = (img - img.min()) / (img.max() - img.min() + 1e-5)\n",
    "    ax = axs[i]\n",
    "    ax.imshow(img)\n",
    "    ax.axis(\"off\")\n",
    "    ax.set_title(f\"Random Sample {idx}\")\n",
    "\n",
    "    # Draw predicted and ground truth boxes\n",
    "    output = output[output[:, 4] > 0.5]  # confidence filter\n",
    "    draw_bounding_boxes(ax, target.cpu(), img.shape[:2], classes, color=\"red\")\n",
    "    draw_bounding_boxes(ax, output, img.shape[:2], classes, color=\"lime\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hard_hat_detection",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
