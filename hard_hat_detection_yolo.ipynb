{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b632b1b9",
   "metadata": {},
   "source": [
    "# Safety First - Hard Hat Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d6c0c4f",
   "metadata": {},
   "source": [
    "---\n",
    "## Project Proposal\n",
    "\n",
    "### Team Member\n",
    "\n",
    "- Stanley Yang\n",
    "- Lennard Vanderspek \n",
    " \n",
    "### Description\n",
    "\n",
    "Workplace safety is very important but often overlooked. Most important way to keep workers \n",
    "safe is by wearing a hard hat. This project will utilize Convolution Neural Net and YOLO to help \n",
    "detect whether workers are wearing a hard hat or not.\n",
    "\n",
    "#### Machine Learning Topics Used:\n",
    "\n",
    "- Convolution Neural Net \n",
    "- YOLO \n",
    " \n",
    "### Expected Outcome\n",
    "\n",
    "A trained model that is able to detect whether workers are wearing a hard hat or not by \n",
    "implementing YOLO using PyTorch. We are hoping to utilize the model to bring more awareness \n",
    "to workplace safety."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c20f2620",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Use CUDA if Available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "922a8ad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# check if CUDA is available\n",
    "train_on_gpu = torch.cuda.is_available()\n",
    "\n",
    "if not train_on_gpu:\n",
    "    print(\"CUDA is not available.  Training on CPU ...\")\n",
    "else:\n",
    "    print(f\"CUDA is available!  Training on GPU ({torch.cuda.get_device_name(0)}) ...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01756944",
   "metadata": {},
   "source": [
    "---\n",
    "## Load the Data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "926ee1bd",
   "metadata": {},
   "source": [
    "### Create Custom Dataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0690d23d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "import cv2\n",
    "import torchvision.transforms as transforms\n",
    "import os\n",
    "\n",
    "# specify the image classes\n",
    "classes = ['head', 'helmet', 'person']\n",
    "num_classes = len(classes)\n",
    "\n",
    "def yolo_collate_fn(batch):\n",
    "    images, labels = zip(*batch)\n",
    "    images = torch.stack(images, dim=0)\n",
    "    # Pad labels to the max number of objects in the batch\n",
    "    max_objs = max(label.shape[0] for label in labels)\n",
    "    padded_labels = []\n",
    "    for label in labels:\n",
    "        pad_size = max_objs - label.shape[0]\n",
    "        if pad_size > 0:\n",
    "            # Pad with -1\n",
    "            padded = torch.nn.functional.pad(label, (0, 0, 0, pad_size), value=-1)\n",
    "        else:\n",
    "            padded = label\n",
    "        padded_labels.append(padded)\n",
    "    labels = torch.stack(padded_labels, dim=0)\n",
    "    return images, labels\n",
    "\n",
    "class custom_dataset(Dataset):\n",
    "    def __init__(self, image_path, label_path, transform:transforms = None, number_image_to_load=None):\n",
    "        self.image_path = image_path\n",
    "        self.label_path = label_path\n",
    "        self.transform = transform\n",
    "\n",
    "        if number_image_to_load is None:\n",
    "            # Load images and labels\n",
    "            self.images = sorted(os.listdir(image_path))\n",
    "            self.labels = sorted(os.listdir(label_path))\n",
    "        else:\n",
    "            # Load a limited number of images and labels\n",
    "            self.images = sorted(os.listdir(image_path))[:number_image_to_load]\n",
    "            self.labels = sorted(os.listdir(label_path))[:number_image_to_load]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image_name = self.images[index]\n",
    "        label_name = self.labels[index]\n",
    "\n",
    "        # Ensure the label file corresponds to the image file\n",
    "        if not label_name.startswith(image_name.split('.')[0]):\n",
    "            raise ValueError(f\"Label file {label_name} does not match image file {image_name}\")\n",
    "\n",
    "        # Load the image\n",
    "        image = cv2.imread(os.path.join(self.image_path, image_name))\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        # Load the label\n",
    "        # Example INPUT with format [class_id, x_center, y_center, width, height]\n",
    "        # 1 0.502 0.6506024096385542 0.032 0.060240963855421686\n",
    "        #\n",
    "        # Example converted OUTPUT with format [x_center, y_center, width, height, confidence, class_0, class_1, class_2]\n",
    "        # 0.502 0.6506024096385542 0.032 0.060240963855421686 1 0.0 1.0 0.0\n",
    "        labels = []\n",
    "        with open(os.path.join(self.label_path, label_name), 'r') as f:\n",
    "            for line in f.readlines():\n",
    "                parts = line.strip().split()\n",
    "                if len(parts) != 5:\n",
    "                    print(f\"Skipping invalid label line: {line.strip()}\")\n",
    "                    continue\n",
    "                class_id = int(parts[0])\n",
    "                x_center = float(parts[1])\n",
    "                y_center = float(parts[2])\n",
    "                width = float(parts[3])\n",
    "                height = float(parts[4])\n",
    "\n",
    "                # Confidence = 1.0 for ground truth\n",
    "                conf = 1.0\n",
    "\n",
    "                # One-hot encoding for class\n",
    "                one_hot_class = [0.0] * num_classes\n",
    "                one_hot_class[class_id] = 1.0\n",
    "\n",
    "                # Final label vector\n",
    "                labels.append([x_center, y_center, width, height, conf] + one_hot_class)\n",
    "\n",
    "        return image, torch.tensor(labels, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52d11681",
   "metadata": {},
   "source": [
    "### Load the Data using Custom Dataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "410e0444",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "\n",
    "# TODO: Need to adjust the default values\n",
    "\n",
    "# number of subprocesses to use for data loading\n",
    "num_workers = 0\n",
    "\n",
    "# how many samples per batch to load\n",
    "batch_size = 64\n",
    "\n",
    "# percentage of training set to use as validation\n",
    "valid_size = 0.2\n",
    "\n",
    "# number of data to load (None for all)\n",
    "train_data_to_load = None\n",
    "test_data_to_load = None\n",
    "\n",
    "# resize the images to this size\n",
    "# smaller size means faster training\n",
    "# larger size means better detection on small objects\n",
    "#\n",
    "# if changed, need to update linear layer input size in the model\n",
    "image_size = (300, 300)\n",
    "\n",
    "# convert data to a normalized torch.FloatTensor\n",
    "transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "        transforms.Resize(image_size),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# load the data set\n",
    "train_data = custom_dataset(\n",
    "    image_path=\"data-yolo-v7/train/images\",\n",
    "    label_path=\"data-yolo-v7/train/labels\",\n",
    "    transform=transform,\n",
    "    number_image_to_load=train_data_to_load,\n",
    ")\n",
    "test_data = custom_dataset(\n",
    "    image_path=\"data-yolo-v7/test/images\",\n",
    "    label_path=\"data-yolo-v7/test/labels\",\n",
    "    transform=transform,\n",
    "    number_image_to_load=test_data_to_load,\n",
    ")\n",
    "num_train = len(train_data)\n",
    "indices = list(range(num_train))\n",
    "np.random.shuffle(indices)\n",
    "split = int(np.floor(valid_size * num_train))\n",
    "train_idx, valid_idx = indices[split:], indices[:split]\n",
    "\n",
    "# define samplers for obtaining training and validation batches\n",
    "train_sampler = SubsetRandomSampler(train_idx)\n",
    "valid_sampler = SubsetRandomSampler(valid_idx)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_data,\n",
    "    batch_size=batch_size,\n",
    "    sampler=train_sampler,\n",
    "    num_workers=num_workers,\n",
    "    collate_fn=yolo_collate_fn,\n",
    "    pin_memory=train_on_gpu,\n",
    ")\n",
    "valid_loader = torch.utils.data.DataLoader(\n",
    "    train_data,\n",
    "    batch_size=batch_size,\n",
    "    sampler=valid_sampler,\n",
    "    num_workers=num_workers,\n",
    "    collate_fn=yolo_collate_fn,\n",
    "    pin_memory=train_on_gpu,\n",
    ")\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    test_data,\n",
    "    batch_size=batch_size,\n",
    "    num_workers=num_workers,\n",
    "    collate_fn=yolo_collate_fn,\n",
    "    pin_memory=train_on_gpu,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a467dbc0",
   "metadata": {},
   "source": [
    "### Custom Bounding Box Drawing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0d51626",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.patches as patches\n",
    "\n",
    "\n",
    "def draw_bounding_boxes(ax, labels, img_size, class_names, color=\"lime\"):\n",
    "    h, w = img_size\n",
    "\n",
    "    for box in labels:\n",
    "        if box[0] == -1:\n",
    "            continue  # skip padding\n",
    "\n",
    "        x_center, y_center, box_w, box_h = box[:4]\n",
    "        class_id = torch.argmax(box[5:]).item()\n",
    "\n",
    "        # Convert from normalized center coords to top-left corner\n",
    "        x = (x_center - box_w / 2) * w\n",
    "        y = (y_center - box_h / 2) * h\n",
    "        width = box_w * w\n",
    "        height = box_h * h\n",
    "\n",
    "        # Draw rectangle\n",
    "        rect = patches.Rectangle(\n",
    "            (x, y), width, height, linewidth=2, edgecolor=color, facecolor=\"none\"\n",
    "        )\n",
    "        ax.add_patch(rect)\n",
    "\n",
    "        # Add label text\n",
    "        label_text = f\"{class_names[class_id]} {box[4].item():.2f}\"\n",
    "        ax.text(\n",
    "            x,\n",
    "            y - 5,\n",
    "            label_text,\n",
    "            color=color,\n",
    "            backgroundcolor=\"black\",\n",
    "            fontsize=10,\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7c16ab5",
   "metadata": {},
   "source": [
    "### Visualize a Batch of Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd3146d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# helper function to un-normalize and display an image\n",
    "def imshow(img):\n",
    "    img = img / 2 + 0.5  # unnormalize\n",
    "    plt.imshow(np.transpose(img, (1, 2, 0)))  # convert from Tensor image\n",
    "\n",
    "# obtain one batch of training images\n",
    "dataiter = iter(train_loader)\n",
    "\n",
    "#images, labels = dataiter.next() #python, torchvision version match issue\n",
    "images, labels = next(dataiter)\n",
    "images = images.numpy() # convert images to numpy for display\n",
    "\n",
    "# plot the images in the batch, along with the corresponding labels\n",
    "fig = plt.figure(figsize=(25, 25))\n",
    "\n",
    "# display 2 images\n",
    "for idx in range(2):\n",
    "    ax = fig.add_subplot(1, 2, idx+1, xticks=[], yticks=[])\n",
    "    image_np = np.transpose(images[idx], (1, 2, 0))  # shape: (H, W, C)\n",
    "    image_np = image_np * 0.5 + 0.5  # unnormalize\n",
    "    ax.imshow(image_np)\n",
    "\n",
    "    label = labels[idx]\n",
    "    valid_labels = label[~(label == -1).all(dim=1)]\n",
    "    draw_bounding_boxes(ax, valid_labels, img_size=(image_np.shape[0], image_np.shape[1]), class_names=classes)\n",
    "\n",
    "    # filter out padding (rows where all values are -1)\n",
    "\n",
    "    \n",
    "    valid_labels = label[~(label == -1).all(axis=1)]\n",
    "    ax.set_title(f'Labels:\\n{valid_labels}')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8a7eed3",
   "metadata": {},
   "source": [
    "### View an Image in More Detail\n",
    "\n",
    "Showing the normalized red, green, and blue (RGB) color channels as three separate, gray scale intensity images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acb65f3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "rgb_img = np.squeeze(images[0])\n",
    "channels = [\"red channel\", \"green channel\", \"blue channel\"]\n",
    "step = 10  # visualizing in steps of 10 pixels due to large image size\n",
    "\n",
    "fig = plt.figure(figsize=(36, 36))\n",
    "for idx in np.arange(rgb_img.shape[0]):\n",
    "    ax = fig.add_subplot(1, 3, idx + 1)\n",
    "    img = rgb_img[idx]\n",
    "    ax.imshow(img, cmap=\"gray\")\n",
    "    ax.set_title(channels[idx])\n",
    "    width, height = img.shape\n",
    "    thresh = img.max() / 2.5\n",
    "    for x in range(0, width, step):\n",
    "        for y in range(0, height, step):\n",
    "            val = round(img[x][y], 2) if img[x][y] != 0 else 0\n",
    "            ax.annotate(\n",
    "                str(val),\n",
    "                xy=(y, x),\n",
    "                horizontalalignment=\"center\",\n",
    "                verticalalignment=\"center\",\n",
    "                size=8,\n",
    "                color=\"white\" if img[x][y] < thresh else \"black\",\n",
    "            )\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8b24e60",
   "metadata": {},
   "source": [
    "---\n",
    "## Define the Network Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42cc2079",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "# TOD: adjust the default values\n",
    "NUMBER_ANCHORS = 3\n",
    "GRID_SIZE = 7\n",
    "\n",
    "class yolo(nn.Module):\n",
    "    # TODO: adjust the default values\n",
    "    def __init__(self, num_anchors=NUMBER_ANCHORS, grid_size=GRID_SIZE, num_classes=num_classes):\n",
    "        super(yolo, self).__init__()\n",
    "\n",
    "        self.num_anchors = num_anchors\n",
    "        self.num_classes = num_classes\n",
    "        self.grid_size = grid_size\n",
    "\n",
    "        self.convolution_layers = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=3),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "\n",
    "            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "\n",
    "            nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.AdaptiveAvgPool2d((self.grid_size, self.grid_size))\n",
    "        )\n",
    "\n",
    "        self.detection_layers = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(256 * grid_size * grid_size, grid_size * grid_size * num_anchors * (5 + num_classes))\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = self.convolution_layers(x)\n",
    "        predictions = self.detection_layers(features)\n",
    "        return predictions.view(-1, self.grid_size, self.grid_size, self.num_anchors, 5 + self.num_classes)\n",
    "\n",
    "\n",
    "\n",
    "# create a complete CNN\n",
    "model = yolo()\n",
    "print(model)\n",
    "\n",
    "# output total number of parameters\n",
    "print(\n",
    "    f\"Total parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad)}\"\n",
    ")\n",
    "\n",
    "# move tensors to GPU if CUDA is available\n",
    "if train_on_gpu:\n",
    "    model.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f2b414f",
   "metadata": {},
   "source": [
    "---\n",
    "## Specify Loss Function and Optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96b6f11c",
   "metadata": {},
   "source": [
    "### Create Custom Loss Function for YOLO\n",
    "\n",
    "YOLO loss function is defined as following ([reference](https://www.geeksforgeeks.org/computer-vision/yolov3-from-scratch-using-pytorch/)):\n",
    "\n",
    "![loss function](./images/yolo_loss_function.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76ae44ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Need to adjust the default values\n",
    "LAMBDA_COORD = 10.0\n",
    "LAMBDA_OBJ = 5.0\n",
    "LAMBDA_NOOBJ = 0.1\n",
    "LAMBDA_CLASS = 1.0\n",
    "\n",
    "def calculate_iou(pred, target):\n",
    "    # pred and target are in [x_center, y_center, w, h]\n",
    "    pred_x1 = pred[0] - pred[2] / 2\n",
    "    pred_y1 = pred[1] - pred[3] / 2\n",
    "    pred_x2 = pred[0] + pred[2] / 2\n",
    "    pred_y2 = pred[1] + pred[3] / 2\n",
    "\n",
    "    target_x1 = target[0] - target[2] / 2\n",
    "    target_y1 = target[1] - target[3] / 2\n",
    "    target_x2 = target[0] + target[2] / 2\n",
    "    target_y2 = target[1] + target[3] / 2\n",
    "\n",
    "    inter_x1 = torch.max(pred_x1, target_x1)\n",
    "    inter_y1 = torch.max(pred_y1, target_y1)\n",
    "    inter_x2 = torch.min(pred_x2, target_x2)\n",
    "    inter_y2 = torch.min(pred_y2, target_y2)\n",
    "\n",
    "    inter_area = (inter_x2 - inter_x1).clamp(0) * (inter_y2 - inter_y1).clamp(0)\n",
    "    pred_area = (pred_x2 - pred_x1) * (pred_y2 - pred_y1)\n",
    "    target_area = (target_x2 - target_x1) * (target_y2 - target_y1)\n",
    "    union = pred_area + target_area - inter_area\n",
    "\n",
    "    # return inter_area / union.clamp(min=1e-6)\n",
    "    return inter_area / (union + 1e-6)\n",
    "\n",
    "def custom_loss(predictions, targets, anchors, confidence_threshold, iou_threshold, lambda_coord=LAMBDA_COORD, lambda_obj=LAMBDA_OBJ, lambda_noobj=LAMBDA_NOOBJ, lambda_class=LAMBDA_CLASS):\n",
    "    batch_size, grid_height, grid_width, num_anchors, pred_len = predictions.shape\n",
    "    device = predictions.device\n",
    "    losses = []\n",
    "\n",
    "    for i in range(batch_size):\n",
    "        matched = []\n",
    "        used_targets = set()\n",
    "        pred = predictions[i]  # shape: [H, W, A, 5+C]\n",
    "        target = targets[i]    # shape: [num_targets, 5+C]\n",
    "\n",
    "        for target_i, target_box in enumerate(target):\n",
    "            gx, gy = target_box[0], target_box[1]\n",
    "            grid_x = int(gx * grid_width)\n",
    "            grid_y = int(gy * grid_height)\n",
    "\n",
    "            best_iou = 0\n",
    "            best_anchor = -1\n",
    "\n",
    "            for anchor_idx in range(num_anchors):\n",
    "                pred_box = pred[grid_y, grid_x, anchor_idx]\n",
    "\n",
    "                # Decode predicted box using anchors\n",
    "                anchor_w, anchor_h = anchors[anchor_idx]\n",
    "                px = torch.sigmoid(pred_box[0])\n",
    "                py = torch.sigmoid(pred_box[1])\n",
    "                pw = torch.exp(pred_box[2]) * anchor_w\n",
    "                ph = torch.exp(pred_box[3]) * anchor_h\n",
    "                pred_box_decoded = torch.tensor([px, py, pw, ph], device=device)\n",
    "\n",
    "                iou = calculate_iou(pred_box_decoded, target_box[:4])\n",
    "                if iou > best_iou:\n",
    "                    best_iou = iou\n",
    "                    best_anchor = anchor_idx\n",
    "\n",
    "            if best_iou > iou_threshold and best_anchor != -1:\n",
    "                matched.append((grid_y, grid_x, best_anchor, target_i))\n",
    "                used_targets.add(target_i)\n",
    "\n",
    "        for gy in range(grid_height):\n",
    "            for gx in range(grid_width):\n",
    "                for a in range(num_anchors):\n",
    "                    pred_box = pred[gy, gx, a]\n",
    "                    pred_conf = torch.sigmoid(pred_box[4])\n",
    "\n",
    "                    match = next(((my, mx, ma, ti) for (my, mx, ma, ti) in matched if my == gy and mx == gx and ma == a), None)\n",
    "\n",
    "                    if match:\n",
    "                        _, _, _, target_i = match\n",
    "                        target_box = target[target_i]\n",
    "\n",
    "                        # Decode prediction\n",
    "                        anchor_w, anchor_h = anchors[a]\n",
    "                        px = torch.sigmoid(pred_box[0])\n",
    "                        py = torch.sigmoid(pred_box[1])\n",
    "                        pw = torch.exp(pred_box[2]) * anchor_w\n",
    "                        ph = torch.exp(pred_box[3]) * anchor_h\n",
    "                        pred_box_decoded = torch.stack([px, py, pw, ph])\n",
    "\n",
    "                        # Coord loss (for tx, ty, tw, th)\n",
    "                        box_loss = (\n",
    "                            torch.nn.functional.mse_loss(px, target_box[0]) +\n",
    "                            torch.nn.functional.mse_loss(py, target_box[1]) +\n",
    "                            torch.nn.functional.mse_loss(torch.sqrt(pw + 1e-6), torch.sqrt(target_box[2] + 1e-6)) +\n",
    "                            torch.nn.functional.mse_loss(torch.sqrt(ph + 1e-6), torch.sqrt(target_box[3] + 1e-6))\n",
    "                        )\n",
    "\n",
    "                        # Object loss\n",
    "                        iou_score = calculate_iou(pred_box_decoded, target_box[:4]).detach()\n",
    "                        obj_loss = torch.nn.functional.mse_loss(pred_conf, iou_score)\n",
    "\n",
    "                        # Class loss\n",
    "                        class_loss = torch.nn.functional.binary_cross_entropy(torch.sigmoid(pred_box[5:]), target_box[5:])\n",
    "\n",
    "                        # print(f\"Loss breakdown - Box: {lambda_coord * box_loss:.4f}, Obj: {lambda_obj * obj_loss:.4f}, Class: {lambda_class * class_loss:.4f}\")\n",
    "\n",
    "                        losses.append(lambda_coord * box_loss + lambda_obj * obj_loss + lambda_class * class_loss)\n",
    "                    else:\n",
    "                        noobj_loss = torch.nn.functional.mse_loss(pred_conf, torch.tensor(0.0, device=device))\n",
    "                        losses.append(lambda_noobj * noobj_loss)\n",
    "\n",
    "    total_loss = torch.stack(losses).sum() / batch_size if losses else torch.tensor(0.0, device=device)\n",
    "    assert total_loss.requires_grad, \"Loss does not require grad!\"\n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5672cefd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import ExponentialLR\n",
    "\n",
    "# specify loss function (categorical cross-entropy)\n",
    "criterion = custom_loss\n",
    "\n",
    "# specify optimizer\n",
    "# TODO: adjust the learning rate\n",
    "# smaller lr means it requires more epochs to improve\n",
    "# larger lr means it may overshoot and become unstable\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "scheduler = ExponentialLR(optimizer, gamma=0.95)    # adding warmup and decay to the epochs to avoid overshooting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e444a4ec",
   "metadata": {},
   "source": [
    "---\n",
    "## Train the Network\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ad01e97",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_loss_min = np.inf  # track change in validation loss\n",
    "total_epochs_run = 0\n",
    "\n",
    "# TODO: Need to adjust base on the anchor size\n",
    "anchors = torch.tensor([\n",
    "    [0.10, 0.15],  # Small object\n",
    "    [0.20, 0.30],  # Medium object\n",
    "    [0.40, 0.50]   # Larger object\n",
    "], dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9752d8ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from datetime import timedelta\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# TODO: update the following variable\n",
    "n_epochs = 10   # number of epochs to run for this iteration\n",
    "warmup_steps = 500  # number of batch steps before the learning rate is back to the base value\n",
    "\n",
    "global_step = total_epochs_run * len(train_loader)\n",
    "base_lr = scheduler.get_last_lr()[0]\n",
    "\n",
    "# load the model if this is a continuation of training\n",
    "if total_epochs_run > 0:\n",
    "    print(f\"Continuing training from epoch {total_epochs_run + 1}\")\n",
    "    model.load_state_dict(torch.load(\"model_trained.pt\", weights_only=True))\n",
    "\n",
    "for epoch in range(1, n_epochs + 1):\n",
    "    print(f\"Epoch {total_epochs_run + epoch}/{total_epochs_run + n_epochs}\")\n",
    "\n",
    "    # TODO: adjust early and late thresholds\n",
    "    CONFIDENCE_THRESHOLD = 0.05\n",
    "    IOU_THRESHOLD = 0.05\n",
    "    if total_epochs_run + epoch > 10:\n",
    "        CONFIDENCE_THRESHOLD = 0.2\n",
    "        IOU_THRESHOLD = 0.2\n",
    "\n",
    "    # keep track of training and validation loss\n",
    "    train_loss = 0.0\n",
    "    valid_loss = 0.0\n",
    "\n",
    "    ###################\n",
    "    # train the model #\n",
    "    ###################\n",
    "    print(\"Training...\")\n",
    "    model.train()\n",
    "    for batch_idx, (images, targets) in enumerate(train_loader):\n",
    "        # add batch lerning rate warmup to the early batches to help with the stability of the training\n",
    "        # Batch-level learning rate warmup using global step\n",
    "        if global_step < warmup_steps:\n",
    "            warmup_lr = base_lr * (global_step + 1) / warmup_steps\n",
    "            for param_group in optimizer.param_groups:\n",
    "                param_group['lr'] = warmup_lr\n",
    "            # print(f\"Global Step {global_step} | Warmup LR: {warmup_lr:.6f}\")\n",
    "        else:\n",
    "            # Reset to scheduler's LR if needed\n",
    "            for param_group in optimizer.param_groups:\n",
    "                param_group['lr'] = scheduler.get_last_lr()[0]\n",
    "\n",
    "        global_step += 1\n",
    "\n",
    "        # move tensors to GPU if CUDA is available\n",
    "        if train_on_gpu:\n",
    "            images, targets = images.cuda(), targets.cuda()\n",
    "\n",
    "        # clear the gradients of all optimized variables\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward pass: compute predicted outputs by passing inputs to the model\n",
    "        output = model(images)\n",
    "\n",
    "        # calculate the batch loss\n",
    "        loss = criterion(output, targets, anchors, CONFIDENCE_THRESHOLD, IOU_THRESHOLD)\n",
    "        print(f\"Batch {batch_idx} | Loss: {loss.item()}\")\n",
    "        \n",
    "        # before = model.detection_layers[1].weight.clone().detach()\n",
    "\n",
    "        # backward pass: compute gradient of the loss with respect to model parameters\n",
    "        loss.backward(retain_graph=True)\n",
    "\n",
    "        # for name, param in model.named_parameters():\n",
    "        #     if param.grad is not None:\n",
    "        #         print(f\"{name}: grad mean = {param.grad.abs().mean().item():.6f}\")\n",
    "        #     else:\n",
    "        #         print(f\"{name}: grad is None\")\n",
    "\n",
    "        # perform a single optimization step (parameter update)\n",
    "        optimizer.step()\n",
    "\n",
    "        # after = model.detection_layers[1].weight.clone().detach()\n",
    "        # print(torch.equal(before, after))\n",
    "        # diff = (before - after).abs().mean()\n",
    "        # print(f\"Weight diff: {diff.item():.6f}\")\n",
    "\n",
    "        # update training loss\n",
    "        train_loss += loss.item() * images.size(0)\n",
    "\n",
    "    ######################\n",
    "    # validate the model #\n",
    "    ######################\n",
    "    print(\"Validating...\")\n",
    "    model.eval()\n",
    "    for batch_idx, (images, targets) in enumerate(valid_loader):\n",
    "        # move tensors to GPU if CUDA is available\n",
    "        if train_on_gpu:\n",
    "            images, targets = images.cuda(), targets.cuda()\n",
    "\n",
    "        # forward pass: compute predicted outputs by passing inputs to the model\n",
    "        output = model(images)\n",
    "\n",
    "        # calculate the batch loss\n",
    "        loss = criterion(output, targets, anchors, CONFIDENCE_THRESHOLD, IOU_THRESHOLD)\n",
    "\n",
    "        # update average validation loss\n",
    "        valid_loss += loss.item() * images.size(0)\n",
    "\n",
    "    # calculate average losses\n",
    "    train_loss = train_loss / len(train_loader.sampler)\n",
    "    valid_loss = valid_loss / len(valid_loader.sampler)\n",
    "\n",
    "    # print training/validation statistics\n",
    "    print(f\"Training Loss: {train_loss}\")\n",
    "    print(f\"Validation Loss: {valid_loss}\")\n",
    "\n",
    "    # save model if validation loss has decreased\n",
    "    if valid_loss > 0 and valid_loss <= valid_loss_min:\n",
    "        print(f\"Validation loss has decreased from {valid_loss_min} to {valid_loss}\")\n",
    "        print(\"Saving model ...\")\n",
    "        torch.save(model.state_dict(), \"model_trained.pt\")\n",
    "        valid_loss_min = valid_loss\n",
    "\n",
    "    scheduler.step()\n",
    "    # print(f\"Learning rate: {scheduler.get_last_lr()[0]}\")\n",
    "    print(\"\\n\")\n",
    "\n",
    "total_epochs_run += n_epochs\n",
    "\n",
    "end_time = time.time()\n",
    "total_duration = end_time - start_time\n",
    "print(f\"Total Time for Training: {str(timedelta(seconds=int(total_duration)))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db9cca04",
   "metadata": {},
   "source": [
    "---\n",
    "## Load the Model with the Lowest Validation Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad5b40eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load('model_trained.pt', weights_only=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a51e78dd",
   "metadata": {},
   "source": [
    "---\n",
    "## Test the Trained Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fba5d97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# track test loss\n",
    "test_loss = 0.0\n",
    "class_correct = [0. for _ in range(num_classes)]\n",
    "class_total = [0. for _ in range(num_classes)]\n",
    "\n",
    "model.eval()\n",
    "for batch_idx, (data, target) in enumerate(test_loader):\n",
    "    if train_on_gpu:\n",
    "        data, target = data.cuda(), target.cuda()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = model(data)\n",
    "\n",
    "    # calculate the batch loss\n",
    "    loss = criterion(output, target)\n",
    "    test_loss += loss.item() * data.size(0)\n",
    "\n",
    "    batch_size = data.size(0)\n",
    "\n",
    "    for b in range(batch_size):\n",
    "        preds = output[b]       # shape: [N, 5 + num_classes]\n",
    "        labels = target[b]      # shape: [M, 5 + num_classes]\n",
    "\n",
    "        labels = labels[labels[:, 4] != -1]  # remove padding\n",
    "\n",
    "        for gt in labels:\n",
    "            gt_class = torch.argmax(gt[5:]).item()\n",
    "            best_iou = 0\n",
    "            best_pred_class = None\n",
    "\n",
    "            filtered_preds = preds[preds[:, 4] > CONFIDENCE_THRESHOLD]\n",
    "\n",
    "            for pred in filtered_preds:\n",
    "                iou = calculate_iou(pred[:4], gt[:4])\n",
    "                if iou > best_iou:\n",
    "                    best_iou = iou\n",
    "                    best_pred_class = torch.argmax(pred[5:]).item()\n",
    "\n",
    "            if best_iou > IOU_THRESHOLD:\n",
    "                class_total[gt_class] += 1\n",
    "                if best_pred_class == gt_class:\n",
    "                    class_correct[gt_class] += 1\n",
    "\n",
    "# average test loss\n",
    "test_loss = test_loss / len(test_loader.dataset)\n",
    "print('Test Loss: {:.6f}\\n'.format(test_loss))\n",
    "\n",
    "for i in range(num_classes):\n",
    "    if class_total[i] > 0:\n",
    "        print('Test Accuracy of %5s: %2d%% (%2d/%2d)' % (\n",
    "            classes[i], 100 * class_correct[i] / class_total[i],\n",
    "            class_correct[i], class_total[i]))\n",
    "    else:\n",
    "        print('Test Accuracy of %5s: N/A (no ground truth samples)' % (classes[i]))\n",
    "\n",
    "if sum(class_total) > 0:\n",
    "    print('\\nTest Accuracy (Overall): %2d%% (%2d/%2d)' % (\n",
    "        100. * sum(class_correct) / sum(class_total),\n",
    "        sum(class_correct), sum(class_total)))\n",
    "else:\n",
    "    print('\\nTest Accuracy (Overall): N/A (no valid ground truth boxes in test set)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "355f924e",
   "metadata": {},
   "source": [
    "## Visualize Sample Test Results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "368742d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "# Collect all test samples into memory for random access\n",
    "all_images = []\n",
    "all_targets = []\n",
    "\n",
    "for images, targets in test_loader:\n",
    "    all_images.extend(images)\n",
    "    all_targets.extend(targets)\n",
    "\n",
    "# Choose N random indices\n",
    "N = 4\n",
    "indices = random.sample(range(len(all_images)), k=N)\n",
    "\n",
    "fig, axs = plt.subplots(1, N, figsize=(5 * N, 5))\n",
    "if N == 1:\n",
    "    axs = [axs]\n",
    "\n",
    "model.eval()\n",
    "for i, idx in enumerate(indices):\n",
    "    image = all_images[idx].unsqueeze(0).to(next(model.parameters()).device)\n",
    "    target = all_targets[idx].to(image.device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = model(image)[0].cpu()\n",
    "\n",
    "    img = image[0].cpu().permute(1, 2, 0).numpy()\n",
    "    img = (img - img.min()) / (img.max() - img.min() + 1e-5)\n",
    "    ax = axs[i]\n",
    "    ax.imshow(img)\n",
    "    ax.axis(\"off\")\n",
    "    ax.set_title(f\"Random Sample {idx}\")\n",
    "\n",
    "    # Draw predicted and ground truth boxes\n",
    "    output = output[output[:, 4] > 0.5]  # confidence filter\n",
    "    draw_bounding_boxes(ax, target.cpu(), img.shape[:2], classes, color=\"red\")\n",
    "    draw_bounding_boxes(ax, output, img.shape[:2], classes, color=\"lime\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hard_hat_detection",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
